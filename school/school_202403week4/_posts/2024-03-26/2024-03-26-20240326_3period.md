---
title: "2024년 03월 26일 3교시 요약"
tag: 202403_school
---

## 인공지능(AI)

- 지도학습 : 문제(x_data) 정답(label, target, y_data)

- 평수(x1), 방수(x2), 화장실...가격(y)
- y' = w1x1 + w2x2 + ...
- w1 = 가중치
- y - y' = 오차(Error, Loss), MSE(Mean Squared Error), MAE(Mean Absolute Error)
- **MSE(Mean Squared Error)** : 에러에 제곱하여 에러의 합이 0이 되는 일을 막고 학습의 효과를 높인다.
- **MAE(Mean Absolute Error)** : 에러를 절대값으로 만들어 에러의 합이 0이 되는 일을 막고 음수를 배제한다.
- 에러의 평균값 : -2, 3, -1, 2, 5, -7
- 에러를 보고 가중치(weight)를 조절하는 과정을 반복한다.
- 각각의 가중치가 에러에 어느정도 기여했는지 계산 : **편미분**
- **역전파(Back Propagation)** : 편미분으로 계산한 결과를 이용하여 가중치를 갱신
- 가중치를 증가시킬지 감소시킬지 결정할 때는 미분(접선의 기울기)을 사용한다. 
  - 기울기가 0보다 작으면 가중치를 키우고 기울기가 0보다 크면 가중치를 줄인다.
  - 결과적으로 경사가 가장 낮은쪽으로 향해간다.
  - **경사하강법** 이라고 한다.
  - 경사하강법이 기본으로 이후 다양한 알고리즘이 파생되었는데 **확률적 경사하강법(SDG)**이 많이 사용된다.
- **순전파** : 가중치를 넣어서 값을 계산하는 과정
- 순전파-역전파 한 세트를 Epoch 라고 한다.
- **학습률(Learning_rate)** : Epoch를 반복하다보면 기울기가 0인 부분을 맴돌기만 할 수 있다. 
  - 그래서 학습률을 설정하여 기울기가 0이 되어갈수록 학습률을 낮추고 기울기가 0에서 멀수록 학습률을 높인다.
- **Adam** : 위와 같은 원리로 이용되는 것이 Adam 알고리즘이다.

---

### 신경망

- 신경망에는 **Layer**가 존재한다.
- Layer에 x1, x2와 같은 데이터가 들어오고 이 데이터가 들어오는 입구를 unit이라고 한다. 
- 따라서 데이터의 수에 따라 unit 이 생성된다.
- Input Layer -> Hidden Layer -> Output Layer
- **Input Layer의 데이터가 Hidden Layer에 전달**될 때 **가중치와 편향값**이 붙어 전달된다.
- **y=w1*x1+b**
  - y = 출력값
  - w1 = 가중치, 기울기 결정
  - x1 = 주어지는 값
  - b = 편향값, 절편
- (w1*x1+w2*x2)...+b
- (w 행) * (x 열) + b
- 신경망에서는 행열을 Tensor 라고 부른다. 
- Tensor 을 이용한 라이브러리로 Tensor flow(구글), Pytorch(페이스북) 등이 있다.
- **Hidden Layer 의 데이터가 Output Layer에 전달**할 때 **출력값을 조정**하는 역할을 하는 것이 **Activation 함수**이다.

#### 클래스

- Input Layer : Flatten
- Hidden Layer,Output Layer : Dense(전결합층)